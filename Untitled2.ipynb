{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torchvision import transforms, utils\n",
    "import os\n",
    "import datetime \n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "##文件数据行为: x   y   z  time     ；表示一个坐标点的三个坐标分量和 采集时间 ，使用空格符分隔\n",
    "\n",
    "def pre(source, distance, diantance1):\n",
    "    f = open(source, \"r\")  # 源文件\n",
    "    fwrit = open(distance, \"a\")  # 卡2068\n",
    "    for s in f.readlines():\n",
    "        if len(s) == 1:\n",
    "            fwrit.write(s)\n",
    "        else:\n",
    "            s = s[:-1]\n",
    "            tem = s.split()\n",
    "            re = tem[1].split(':')[1] + \"\\t\" + tem[2].split(':')[1] + \"\\t\" + tem[3].split(':')[1] + \"\\t\" + tem[-2] + \"\\t\" + tem[-1] + \"\\n\"\n",
    "            # if tem[0] != \"2068\":\n",
    "            #     fwrit1.write(re)\n",
    "            # else:\n",
    "            if tem[0] == \"2068\":\n",
    "                fwrit.write(re)\n",
    "    f.close()\n",
    "    fwrit.close()\n",
    "\n",
    "\n",
    "def file_name(file_dir, target, target1):\n",
    "    path = [file_dir + '\\\\' + x for x in os.listdir(file_dir)]\n",
    "    for p in path:\n",
    "        if not os.path.isdir(p):\n",
    "            pre(p, target, target1)\n",
    "\n",
    "def split_data(splot):\n",
    "    \"\"\"\n",
    "    按照plot划分时间段\n",
    "    \"\"\"\n",
    "    state = \"D:\\\\fig\\\\data\\\\pre2068Static.txt\"\n",
    "    unrealize = \"D:\\\\fig\\\\data\\\\pre2068Unrealize.txt\"\n",
    "    Sactive = \"D:\\\\fig\\\\data\\\\pre2068Little.txt\"\n",
    "    Mactive = \"D:\\\\fig\\\\data\\\\pre2068LargeMove.txt\"\n",
    "    files = [state, unrealize, Sactive, Mactive]\n",
    "    mask = [0., 1., 2., 3.]\n",
    "    splotre = []\n",
    "    lable = [] \n",
    "    for index, file in enumerate(files):\n",
    "        f = open(file, \"r\")\n",
    "        mk = mask[index]\n",
    "\n",
    "        flag = False \n",
    "        start = ''\n",
    "        obj = [] \n",
    "        for s in f.readlines():\n",
    "            if len(s) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                s = s[:-1]\n",
    "                seq = s.split(\"\\t\")\n",
    "                if flag == False :\n",
    "                    #本数据序列第一点的采集时间\n",
    "                    start = 0\n",
    "                    now = 0\n",
    "                    flag = True \n",
    "                # 当前点的采集时间\n",
    "                now = now + 1\n",
    "                subt = now - start \n",
    "                obj.append(np.asarray(seq[:3],dtype='float64'))\n",
    "            if subt > splot:\n",
    "                splotre.append(np.asarray(obj))\n",
    "                lable.append(mk)\n",
    "                obj.clear() \n",
    "                flag = False\n",
    "    splotre = np.asarray(splotre)\n",
    "    lable = np.asarray(lable)\n",
    "    return splotre, lable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大幅度运动 原始数据源\n",
    "p = \"E:\\\\datacollect\\\\trian\\\\active\"\n",
    "# 数据预处理结果保存路径\n",
    "t = \"D:\\\\fig\\\\data\\\\pre2068LargeMove.txt\"  # 卡2068对应的数据，处理结果\n",
    "t1 = \"D:\\\\fig\\\\data\\\\preLargeMove.txt\"  # 其他卡的处理结果\n",
    "\n",
    "file_name(p, t, t1)\n",
    "p = \"E:\\\\datacollect\\\\trian\\\\little\"  # 原始数据源\n",
    "\n",
    "# 数据预处理结果保存路径\n",
    "t = \"D:\\\\fig\\\\data\\\\pre2068Little.txt\"  # 卡2068对应的数据，处理结果\n",
    "t1 = \"D:\\\\fig\\\\data\\\\preLittle.txt\"  # 其他卡的处理结果\n",
    "\n",
    "file_name(p, t, t1)\n",
    "p = \"E:\\\\datacollect\\\\trian\\\\static\"  # 原始数据源\n",
    "\n",
    "# 数据预处理结果保存路径\n",
    "t = \"D:\\\\fig\\\\data\\\\pre2068Static.txt\"  # 卡2068对应的数据，处理结果\n",
    "t1 = \"D:\\\\fig\\\\data\\\\preStatic.txt\"  # 其他卡的处理结果\n",
    "file_name(p, t, t1)\n",
    "# 无意识运动，如转身，手摆动\n",
    "p = \"E:\\\\datacollect\\\\trian\\\\unrealize\"  # 原始数据源\n",
    "# 数据预处理结果保存路径\n",
    "t = \"D:\\\\fig\\\\data\\\\pre2068Unrealize.txt\"  # 卡2068对应的数据，处理结果\n",
    "t1 = \"D:\\\\fig\\\\data\\\\preUnrealize.txt\"  # 其他卡的处理结果径\n",
    "file_name(p, t, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splotre, lable = split_data(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = 25000\n",
    "index = list(range(splotre.shape[0]))\n",
    "np.random.shuffle(index)\n",
    "splotre = splotre[index]\n",
    "lable = lable[index]\n",
    "train_splot = splotre[:NUM_TRAIN-1000]\n",
    "train_lable = lable[:NUM_TRAIN-1000]\n",
    "val_splot = splotre[NUM_TRAIN-1000:NUM_TRAIN]\n",
    "val_lable = lable[NUM_TRAIN-1000:NUM_TRAIN]\n",
    "test_splot = splotre[NUM_TRAIN:]\n",
    "test_lable = lable[NUM_TRAIN:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, splot):\n",
    "        return torch.from_numpy(splot)\n",
    "    \n",
    "trans = T.Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "class LocationDataset(Dataset):\n",
    "    def __init__(self, splotre, lable, transform = trans):\n",
    "        self.splotre = np.transpose(splotre, (0, 2, 1))\n",
    "        self.lable = lable \n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.splotre)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        splot = self.splotre[idx] \n",
    "        lable = self.lable[idx]\n",
    "        tensor = trans(splot)\n",
    "        return tensor, lable\n",
    "train_dataset = LocationDataset(train_splot, train_lable)\n",
    "loader_train = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataset = LocationDataset(val_splot, val_lable)\n",
    "loader_val = DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406,\n",
       "          4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406, 4.8406,\n",
       "          4.8406, 4.8406],\n",
       "         [0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679,\n",
       "          0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679, 0.8679,\n",
       "          0.8679, 0.8679],\n",
       "         [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
       "          1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
       "          1.5000, 1.5000]], device='cpu', dtype=torch.float64), 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_val.dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # useful stateless functions\n",
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1.5440\n",
      "Got 223 / 1000 correct (22.30)\n",
      "\n",
      "Iteration 100, loss = 1.3791\n",
      "Got 226 / 1000 correct (22.60)\n",
      "\n",
      "Iteration 200, loss = 1.3519\n",
      "Got 309 / 1000 correct (30.90)\n",
      "\n",
      "Iteration 300, loss = 1.3340\n",
      "Got 348 / 1000 correct (34.80)\n",
      "\n",
      "Iteration 400, loss = 1.3072\n",
      "Got 353 / 1000 correct (35.30)\n",
      "\n",
      "Iteration 500, loss = 1.5129\n",
      "Got 361 / 1000 correct (36.10)\n",
      "\n",
      "Iteration 600, loss = 1.3006\n",
      "Got 436 / 1000 correct (43.60)\n",
      "\n",
      "Iteration 700, loss = 1.1521\n",
      "Got 663 / 1000 correct (66.30)\n",
      "\n",
      "Iteration 800, loss = 1.4510\n",
      "Got 522 / 1000 correct (52.20)\n",
      "\n",
      "Iteration 900, loss = 1.1256\n",
      "Got 462 / 1000 correct (46.20)\n",
      "\n",
      "Iteration 1000, loss = 1.0687\n",
      "Got 563 / 1000 correct (56.30)\n",
      "\n",
      "Iteration 1100, loss = 1.2375\n",
      "Got 631 / 1000 correct (63.10)\n",
      "\n",
      "Iteration 1200, loss = 1.6296\n",
      "Got 666 / 1000 correct (66.60)\n",
      "\n",
      "Iteration 1300, loss = 1.4455\n",
      "Got 653 / 1000 correct (65.30)\n",
      "\n",
      "Iteration 1400, loss = 0.9169\n",
      "Got 643 / 1000 correct (64.30)\n",
      "\n",
      "Iteration 1500, loss = 0.9098\n",
      "Got 552 / 1000 correct (55.20)\n",
      "\n",
      "Iteration 1600, loss = 0.8668\n",
      "Got 551 / 1000 correct (55.10)\n",
      "\n",
      "Iteration 1700, loss = 0.8729\n",
      "Got 701 / 1000 correct (70.10)\n",
      "\n",
      "Iteration 1800, loss = 1.0388\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Iteration 1900, loss = 0.7090\n",
      "Got 730 / 1000 correct (73.00)\n",
      "\n",
      "Iteration 2000, loss = 1.2654\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Iteration 2100, loss = 0.8734\n",
      "Got 755 / 1000 correct (75.50)\n",
      "\n",
      "Iteration 2200, loss = 1.0310\n",
      "Got 710 / 1000 correct (71.00)\n",
      "\n",
      "Iteration 2300, loss = 0.5800\n",
      "Got 673 / 1000 correct (67.30)\n",
      "\n",
      "Iteration 2400, loss = 0.8856\n",
      "Got 773 / 1000 correct (77.30)\n",
      "\n",
      "Iteration 2500, loss = 0.6230\n",
      "Got 746 / 1000 correct (74.60)\n",
      "\n",
      "Iteration 2600, loss = 0.7643\n",
      "Got 771 / 1000 correct (77.10)\n",
      "\n",
      "Iteration 2700, loss = 0.6020\n",
      "Got 780 / 1000 correct (78.00)\n",
      "\n",
      "Iteration 2800, loss = 0.5822\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Iteration 2900, loss = 0.4988\n",
      "Got 733 / 1000 correct (73.30)\n",
      "\n",
      "Iteration 3000, loss = 0.7917\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Iteration 3100, loss = 0.7385\n",
      "Got 812 / 1000 correct (81.20)\n",
      "\n",
      "Iteration 3200, loss = 0.4986\n",
      "Got 796 / 1000 correct (79.60)\n",
      "\n",
      "Iteration 3300, loss = 0.3124\n",
      "Got 819 / 1000 correct (81.90)\n",
      "\n",
      "Iteration 3400, loss = 0.3176\n",
      "Got 847 / 1000 correct (84.70)\n",
      "\n",
      "Iteration 3500, loss = 0.3073\n",
      "Got 705 / 1000 correct (70.50)\n",
      "\n",
      "Iteration 3600, loss = 0.2536\n",
      "Got 807 / 1000 correct (80.70)\n",
      "\n",
      "Iteration 3700, loss = 0.3691\n",
      "Got 769 / 1000 correct (76.90)\n",
      "\n",
      "Iteration 3800, loss = 0.1700\n",
      "Got 796 / 1000 correct (79.60)\n",
      "\n",
      "Iteration 3900, loss = 0.3560\n",
      "Got 820 / 1000 correct (82.00)\n",
      "\n",
      "Iteration 4000, loss = 0.9114\n",
      "Got 753 / 1000 correct (75.30)\n",
      "\n",
      "Iteration 4100, loss = 0.3933\n",
      "Got 839 / 1000 correct (83.90)\n",
      "\n",
      "Iteration 4200, loss = 0.4550\n",
      "Got 836 / 1000 correct (83.60)\n",
      "\n",
      "Iteration 4300, loss = 0.8713\n",
      "Got 807 / 1000 correct (80.70)\n",
      "\n",
      "Iteration 4400, loss = 0.8474\n",
      "Got 832 / 1000 correct (83.20)\n",
      "\n",
      "Iteration 4500, loss = 0.3012\n",
      "Got 843 / 1000 correct (84.30)\n",
      "\n",
      "Iteration 4600, loss = 0.5758\n",
      "Got 849 / 1000 correct (84.90)\n",
      "\n",
      "Iteration 4700, loss = 0.4914\n",
      "Got 843 / 1000 correct (84.30)\n",
      "\n",
      "Iteration 4800, loss = 0.4311\n",
      "Got 864 / 1000 correct (86.40)\n",
      "\n",
      "Iteration 4900, loss = 0.3031\n",
      "Got 838 / 1000 correct (83.80)\n",
      "\n",
      "Iteration 5000, loss = 0.6991\n",
      "Got 843 / 1000 correct (84.30)\n",
      "\n",
      "Iteration 5100, loss = 0.6408\n",
      "Got 844 / 1000 correct (84.40)\n",
      "\n",
      "Iteration 5200, loss = 0.1464\n",
      "Got 842 / 1000 correct (84.20)\n",
      "\n",
      "Iteration 5300, loss = 0.3087\n",
      "Got 847 / 1000 correct (84.70)\n",
      "\n",
      "Iteration 5400, loss = 0.5892\n",
      "Got 797 / 1000 correct (79.70)\n",
      "\n",
      "Iteration 5500, loss = 0.2122\n",
      "Got 835 / 1000 correct (83.50)\n",
      "\n",
      "Iteration 5600, loss = 0.3845\n",
      "Got 793 / 1000 correct (79.30)\n",
      "\n",
      "Iteration 5700, loss = 0.4788\n",
      "Got 840 / 1000 correct (84.00)\n",
      "\n",
      "Iteration 5800, loss = 0.1198\n",
      "Got 800 / 1000 correct (80.00)\n",
      "\n",
      "Iteration 5900, loss = 0.0765\n",
      "Got 855 / 1000 correct (85.50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "channel_1 = 32 \n",
    "channel_2 = 16\n",
    "learning_rate = 1e-4\n",
    "model = None \n",
    "optimizer = None \n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv1d(3, 32, 3, stride = 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(32, 64, 2, stride = 2),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(576, 4)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9, nesterov = True)\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(loader_train):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
